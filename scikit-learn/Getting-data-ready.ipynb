{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting our data ready for ML via sklearn\n",
    "\n",
    "Yeah, I know, it feels kind of difficult... <br>\n",
    "![](https://media1.tenor.com/images/7ecb7f303712e4e24350d5b5ad9689fa/tenor.gif?itemid=5436040)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our objectives \n",
    "#### 1. Split data into features (x usually) and labels (y usually)\n",
    "#### 2. Filling (also called \"Imputing\") or disregarding missing values\n",
    "#### 3. Converting non-numerical values to numerical values (a.k.a Feature encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's begin with the heart disease dataset\n",
    "hd = pd.read_csv(\"https://raw.githubusercontent.com/ineelhere/Machine-Learning-and-Data-Science/master/scikit-learn/heart-disease.csv\")\n",
    "hd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length/size of the dataset\n",
    "len(hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature (x) ready = all columns except target\n",
    "x = hd.drop(\"target\", axis=1)\n",
    "x.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels (y) ready = target column\n",
    "y = hd.target\n",
    "y.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Always Remember! <br>\n",
    "## NEVER  EVALUATE  or  TEST your models on DATA that it is LEARNT FROM -- that's why we split it into training and test sets.\n",
    "(because if you do, it is like cheating in an examination)  <br>\n",
    "![](https://media.giphy.com/media/A4CLbWb9o5rj2/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Splitting the data into test and training sets\n",
    "Imagine it this way\n",
    "* Test data = Final exam\n",
    "* Training data = Mock exam (So we use the train_test_split first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2) \n",
    "# test_size=0.2 means that we want our test dataset to be 20% of the overall data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shapes of the new matrices just created. \n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just notice,  <br>\n",
    "242 = 80% of 303 <br>\n",
    "61 = 20% of 303 <br>\n",
    "So, training set is 80% of overall data and test set is 20% of the overall data  <br>\n",
    "(I have mentioned this above too!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Filling (also called \"Imputing\") or disregarding missing values\n",
    "### Make sure data is in numerial format/values  - if not, make them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us use a new dataset now. So first we will get the dataset\n",
    "cse = pd.read_csv(\"https://raw.githubusercontent.com/ineelhere/Machine-Learning-and-Data-Science/master/scikit-learn/car-sales-extended.csv\")\n",
    "cse.head()\n",
    "#ummm... I mean car-sales-extended by cse. (I'm lazy!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cse.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the feature and label\n",
    "x = cse.drop(\"Price\", axis=1)\n",
    "y = cse.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into test and training sets (Practice time!)\n",
    "from sklearn.model_selection import train_test_split # No need to do it again, it's already done above. But still... :P\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.size, x_test.size, y_train.size, y_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build ML model\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this random forest reggressor is the same as a classifier random forest. <br>\n",
    "But this time it can predict a number which is what we're trying to do. <br>\n",
    "We're trying to predict the price of a car given we are given some attributes about it. <br>\n",
    "\n",
    "#### Now, our ML algorithm cannot deal with strings! So we need to take care of that --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Converting categorical values (strings) into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.compose import ColumnTransformer\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "one_hot = OneHotEncoder() #instantiate the one hot encoder (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "transformer = ColumnTransformer([(\"one_hot\",\n",
    "                                   one_hot,\n",
    "                                   categorical_features)],\n",
    "                                   remainder=\"passthrough\") \n",
    "# this is 'transformer' created by 'columntransformer' and we are asking the columntransformr to take the \n",
    "# onehot encoder and apply it to the categorical features and for the remainder/remaining of the columns - \n",
    "# just passthrough - don't do anything to those!\n",
    "transformed_x = transformer.fit_transform(x) #fit the above to our data = x\n",
    "transformed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(transformed_x) #changing the above into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The concept of \"One hot encoding in python\"\n",
    "![](https://cdn-images-1.medium.com/fit/t/1600/480/1*ggtP4a5YaRx6l09KQaYOnw.png)\n",
    "\"<p>One Hot Encoding is a common way of preprocessing categorical features for machine learning models. This type of encoding creates a new binary feature for each possible category and assigns a value of 1 to the feature of each sample that corresponds to its original category. It&rsquo;s easier to understand visually: in the example below, we One Hot Encode a&nbsp;<em class=\"ji\">color&nbsp;</em>feature which consists of three categories (<em class=\"ji\">red</em>,&nbsp;<em class=\"ji\">green</em>, and&nbsp;<em class=\"ji\">blue</em>).</p>\n",
    "<p>Sci-kit Learn offers the&nbsp;<code class=\"jy kj kk kl km b\">OneHotEncoder</code>&nbsp;class out of the box to handle categorical inputs using One Hot Encoding. Simply create an instance of&nbsp;<code class=\"jy kj kk kl km b\">sklearn.preprocessing.OneHotEncoder</code>&nbsp;then fit the encoder on the input data (this is where the One Hot Encoder identifies the possible categories in the DataFrame and updates some internal state, allowing it to map each category to a unique binary feature), and finally, call&nbsp;<code class=\"jy kj kk kl km b\">one_hot_encoder.transform()</code>&nbsp;to One Hot Encode the input DataFrame. The great thing about the&nbsp;<code class=\"jy kj kk kl km b\">OneHotEncoder</code>&nbsp;class is that, once it has been fit on the input features, you can continue to pass it new samples, and it will encode the categorical features consistently.</p>\"\n",
    "\n",
    "Source of above information - https://towardsdatascience.com/building-a-one-hot-encoding-layer-with-tensorflow-f907d686bf39\n",
    "\n",
    "### Now, there is another way to do this! - Using pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(cse[[\"Make\", \"Colour\", \"Doors\" ]])\n",
    "dummies #doesn't work as above as Doors are in numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's refit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor() #create a model\n",
    "np.random.seed(42)\n",
    "x_train, x_test,  y_train, y_test = train_test_split(transformed_x,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2) #remember to keep the orders same (funny, I mess it up all the time!)\n",
    "model.fit(x_train, y_train) #train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_test, y_test) #run/evaluate a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media1.tenor.com/images/50e1f1cdd0df8e1bd6a7feab86ca8ac8/tenor.gif?itemid=8852977)\n",
    "\n",
    "\n",
    "# Handling Missing Values With Pandas\n",
    "\n",
    "* Imputation - Fill the missing values with 'some' values. \n",
    "* Remove the samples with missing data altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import a new dataset\n",
    "csm = pd.read_csv(\"https://raw.githubusercontent.com/ineelhere/Machine-Learning-and-Data-Science/master/scikit-learn/car-sales-missing-data.csv\")\n",
    "csm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csm.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csm.isna().sum() # get a compact data on no of cells with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same thing with a larger dataset\n",
    "csme = pd.read_csv(\"https://raw.githubusercontent.com/ineelhere/Machine-Learning-and-Data-Science/master/scikit-learn/car-sales-extended-missing-data.csv\")\n",
    "csme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csme.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media1.tenor.com/images/bbb5ed42721b7a95d4507fe68caba984/tenor.gif?itemid=5195597)\n",
    "#### Convert our data to numbers | categories to numbers (just as before). \n",
    "##### But before that, remove the \"NaN\" from the dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option-1 use pandas to fill the missing values\n",
    "\n",
    "# fill the 'Make' column\n",
    "csme.Make.fillna(\"missing\", inplace=True)\n",
    "\n",
    "# fill the 'Colour' column\n",
    "csme.Colour.fillna(\"missing\", inplace=True)\n",
    "\n",
    "# fill the 'Odometer (KM)' column\n",
    "csme['Odometer (KM)'].fillna(csme['Odometer (KM)'].mean(), inplace=True) # replace with the mean\n",
    "\n",
    "# fill the 'Doors' column\n",
    "csme.Doors.fillna(4, inplace=True) # as most cars have 4 doors ideally\n",
    "\n",
    "#lets see what we end up with\n",
    "csme.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, now we need to think about the price!\n",
    "# lets see what is the mean price here - would it be logical to use the mean price in missing sections?\n",
    "csme.Price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nahhh that doesn't look ideal for me!\n",
    "# So I will simply remove the whole rows which have prices missing.\n",
    "# it's just my choice, your's might be different - cheers!\n",
    "csme.dropna(inplace=True)\n",
    "csme.isna().sum()\n",
    "# so we miss 50 rows now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the feature and label\n",
    "x = csme.drop(\"Price\", axis=1)\n",
    "y = csme.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.compose import ColumnTransformer\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "one_hot = OneHotEncoder() #instantiate the one hot encoder (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "transformer = ColumnTransformer([(\"one_hot\",\n",
    "                                   one_hot,\n",
    "                                   categorical_features)],\n",
    "                                   remainder=\"passthrough\") \n",
    "# this is 'transformer' created by 'columntransformer' and we are asking the columntransformr to take the \n",
    "# onehot encoder and apply it to the categorical features and for the remainder/remaining of the columns - \n",
    "# just passthrough - don't do anything to those!\n",
    "\n",
    "# HERE IS A LITTLE CHANGE\n",
    "transformed_x = transformer.fit_transform(csme) #fit the above to the dataset itself INSTEAD of x!\n",
    "transformed_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media1.tenor.com/images/cb84e3e676def7599aec1fca4ed0e461/tenor.gif?itemid=5610528)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Once your data is all in numerical format, there's one more transformation you'll probably want to do to it.</p>\n",
    "<p>It's called&nbsp;<strong>Feature Scaling</strong>.</p>\n",
    "<p>In other words, making sure all of your numerical data is on the same scale.</p>\n",
    "<p>For example, say you were trying to predict the sale price of cars and the number of kilometres on their odometers varies from 6,000 to 345,000 but the median previous repair cost varies from 100 to 1,700. A machine learning algorithm may have trouble finding patterns in these wide-ranging variables.</p>\n",
    "<p>To fix this, there are two main types of feature scaling.</p>\n",
    "<ul>\n",
    "<li>\n",
    "<p><strong>Normalization&nbsp;</strong>(also called min-max scaling) - This rescales all the numerical values to between 0 and 1, with the lowest value being close to 0 and the highest previous value being close to 1. Scikit-Learn provides functionality for this in the&nbsp;<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\" rel=\"noopener noreferrer\">MinMaxScalar class</a>.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><strong>Standardization</strong>&nbsp;- This subtracts the mean value from all of the features (so the resulting features have 0 mean). It then scales the features to unit variance (by dividing the feature by the standard deviation). Scikit-Learn provides functionality for this in the&nbsp;<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\" rel=\"noopener noreferrer\">StandardScalar class</a>.</p>\n",
    "</li>\n",
    "</ul>\n",
    "<p>A couple of things to note.</p>\n",
    "<ul>\n",
    "<li>\n",
    "<p>Feature scaling usually isn't required for your target variable.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p>Feature scaling is usually not required with tree-based models (e.g. Random Forest) since they can handle varying features.</p>\n",
    "</li>\n",
    "</ul>\n",
    "<p><strong>Extra reading</strong></p>\n",
    "<p>For further information on this topic, I'd suggest the following resources.</p>\n",
    "<ul>\n",
    "<li>\n",
    "<p><a href=\"https://medium.com/@rahul77349/feature-scaling-why-it-is-required-8a93df1af310\" rel=\"noopener noreferrer\">Feature Scaling - why is it required?</a>&nbsp;by Rahul Saini</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><a href=\"https://benalexkeen.com/feature-scaling-with-scikit-learn/\" rel=\"noopener noreferrer\">Feature Scaling with Scikit-Learn</a>&nbsp;by Ben Alex Keen</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><a href=\"https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/\" rel=\"noopener noreferrer\">Feature Scaling for Machine Learning: Understanding the Difference Between Normalization vs. Standardization</a>&nbsp;by Aniruddha Bhandari</p>\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "# Handling Missing Values With ScikitLearn\n",
    "\n",
    "<p>Safety tips! :p</p>\n",
    "<ul>\n",
    "<li>\n",
    "<p>Split your data first (into train/test), always keep your training &amp; test data separate</p>\n",
    "</li>\n",
    "<li>\n",
    "<p>Fill/transform the training set and test sets separately (this goes for filling data with pandas as well)</p>\n",
    "</li>\n",
    "<li>\n",
    "<p>Don't use data from the future (test set) to fill data from the past (training set)</p>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import a fresh version of the dataset\n",
    "csm = pd.read_csv(\"https://raw.githubusercontent.com/ineelhere/Machine-Learning-and-Data-Science/master/scikit-learn/car-sales-missing-data.csv\")\n",
    "csm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python38132bit93aab49dff954651a4c1e4402da878fd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
